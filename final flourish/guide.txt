Of course. It is time to finalize the project structure and consolidate our work into the definitive script. Here is the complete guide to structuring your project and the final script for the night lights and disease analysis.

### **1. The Correct Project Folder Structure**

A clean, organized structure is crucial for a project of this complexity. The current layout in the repository is a development state; the structure below is the professional standard for a data science project. It separates data, source code, and outputs, making the project easier to manage and scale.

```
kalopathor_engines/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â”œâ”€â”€ dengue_reports_2022_2024/
â”‚   â”‚   â”‚   â””â”€â”€ pdfs/            # All 1,084 downloaded DGHS PDFs go here
â”‚   â”‚   â””â”€â”€ bbs_census_2022/     # All BBS Census CSVs go here
â”‚   â”‚
â”‚   â””â”€â”€ processed/
â”‚       â””â”€â”€ # Clean, analysis-ready datasets will be saved here
â”‚
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ data_processing/
â”‚       â””â”€â”€ dengue_pdf_extractor.py # The new PDF extractor script
â”‚
â”œâ”€â”€ reports/
â”‚   â””â”€â”€ figures/
â”‚       â””â”€â”€ # Future location for charts and maps
â”‚
â”œâ”€â”€ .gitattributes
â”œâ”€â”€ README.md
â””â”€â”€ run_hawkeye_omega.py         # <-- THE FINAL, MAIN SCRIPT GOES HERE
```

### **2. Instructions to Reorganize Your Project**

You can execute these commands in your Cursor AI terminal to automatically restructure your current project into the correct format.

```bash
# This assumes you are in the root of the 'kalopathor_engines' directory

# 1. Create the new directory structure
mkdir -p data/raw/dengue_reports_2022_2024/pdfs
mkdir -p data/raw/bbs_census_2022
mkdir -p data/processed
mkdir -p scripts/data_processing
mkdir -p reports/figures

# 2. Move the new PDF extractor script to its proper location
mv dengue_pdf_extractor.py scripts/data_processing/

# 3. Move all the downloaded BBS Census data into the new raw data folder
# This command finds all files with 'bangladesh_bbs' in the name and moves them
mv "final flourish/"*bangladesh_bbs*.csv data/raw/bbs_census_2022/

# 4. Move the other key data files into the processed data folder
# These are the files our final script will directly use
mv "final flourish/bangladesh_dengue_cases_2022_2025.csv" data/processed/
mv "final flourish/dhaka_weather_2022_2025.csv" data/processed/
mv "final flourish/dhaka_nightlights_2022_2025.csv" data/processed/
mv "final flourish/bangladesh_population_monthly_2022_2025.csv" data/processed/

# 5. Clean up by moving the rest of the 'final flourish' contents into an archive
mkdir -p "final flourish/archive"
mv "final flourish"/* "final flourish/archive/"

echo "âœ… Project structure has been reorganized successfully."
```

### **3. The Final Analysis Script: `run_hawkeye_omega.py`**

This is the definitive script for the project. It integrates all our work: loading the prepared historical files, fetching live API data, unifying the datasets, discovering causal links, training a predictive model, and generating a final, comprehensive report.

**Action:** Place this file in the root of your newly organized `kalopathor_engines` project.

```python
"""
HAWKEYE OMEGA v2 - FINAL ANALYSIS PIPELINE
===========================================
This is the definitive, integrated script that executes the full vision.
It combines historical data, live API feeds, and pragmatic machine learning
to generate a complete disease-economy nexus analysis for Dhaka.

To Run:
1. Ensure project is structured correctly.
2. Ensure processed data CSVs are in 'data/processed/'.
3. Execute from the project root: python run_hawkeye_omega.py
"""

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import requests
import ee
import json
import networkx as nx
from datetime import datetime, timedelta
from sklearn.preprocessing import MinMaxScaler
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, WhiteKernel, ConstantKernel
import warnings

warnings.filterwarnings('ignore')

# --- CONFIGURATION ---
class Config:
    # API Credentials and Project Info
    GCP_PROJECT_ID = "hyperion-472805"
    OPENWEATHER_API_KEY = "9c18ae394b206b79263dd86628d7ef8b"
    
    # Geographic Area of Interest
    DHAKA_LAT = 23.8103
    DHAKA_LON = 90.4125
    
    # File Paths for Processed Data
    DATA_DIR = "./data/processed/"
    DENGUE_FILE = "bangladesh_dengue_cases_2022_2025.csv"
    WEATHER_FILE = "dhaka_weather_2022_2025.csv"
    NIGHTLIGHT_FILE = "dhaka_nightlights_2022_2025.csv"
    POPULATION_FILE = "bangladesh_population_monthly_2022_2025.csv"
    
    # Model Parameters
    FORECAST_HORIZON = 14  # days
    CAUSAL_LAG = 7         # days
    CORR_THRESHOLD = 0.25  # Minimum correlation to be considered a causal link

# --- DATA MANAGEMENT ---
class DataManager:
    """Handles loading, fetching, and unifying all data sources."""
    def __init__(self, config):
        self.config = config

    def load_historical_data(self):
        """Loads all historical data from the 'data/processed/' CSV files."""
        print("1. Loading historical data from local files...")
        try:
            dengue_df = pd.read_csv(self.config.DATA_DIR + self.config.DENGUE_FILE, parse_dates=['date'])
            weather_df = pd.read_csv(self.config.DATA_DIR + self.config.WEATHER_FILE, parse_dates=['date'])
            nightlight_df = pd.read_csv(self.config.DATA_DIR + self.config.NIGHTLIGHT_FILE, parse_dates=['date'])
            pop_df = pd.read_csv(self.config.DATA_DIR + self.config.POPULATION_FILE, parse_dates=['date'])
            print("âœ“  Successfully loaded historical dengue, weather, nightlight, and population data.")
            return dengue_df, weather_df, nightlight_df, pop_df
        except FileNotFoundError as e:
            print(f"ðŸ”¥ ERROR: A required data file was not found in '{self.config.DATA_DIR}'. Details: {e}")
            print("ðŸ”¥ Please ensure the PDF extractor script has been run and all files are in place.")
            raise

    def fetch_live_data(self):
        """Fetches the latest data points from live APIs."""
        print("2. Fetching live data from APIs...")
        live_weather = self._fetch_live_weather()
        print("âœ“  Successfully fetched live weather data.")
        return live_weather

    def _fetch_live_weather(self):
        url = f'http://api.openweathermap.org/data/2.5/weather?lat={self.config.DHAKA_LAT}&lon={self.config.DHAKA_LON}&appid={self.config.OPENWEATHER_API_KEY}&units=metric'
        try:
            res = requests.get(url, timeout=10)
            res.raise_for_status()
            data = res.json()
            return {
                'date': datetime.utcfromtimestamp(data['dt']).date(),
                'temperature': data['main']['temp'],
                'humidity': data['main']['humidity'],
                'rainfall': data.get('rain', {}).get('1h', 0)
            }
        except Exception as e:
            print(f"âš ï¸  Could not fetch live weather. Using last known values. Details: {e}")
            return None

    def unify_data(self, dengue_df, weather_df, nightlight_df, pop_df, live_weather):
        """Merges all data sources into a single, daily-frequency DataFrame."""
        print("3. Unifying all data streams into a daily timeline...")
        
        df = pd.merge(dengue_df, weather_df, on='date', how='outer')
        df = pd.merge(df, nightlight_df, on='date', how='outer')
        df = pd.merge(df, pop_df, on='date', how='outer')
        df['date'] = pd.to_datetime(df['date'])
        
        if live_weather:
            live_df = pd.DataFrame([live_weather])
            live_df['date'] = pd.to_datetime(live_df['date'])
            df = pd.concat([df, live_df], ignore_index=True)

        df = df.sort_values('date').drop_duplicates(subset=['date'], keep='last').reset_index(drop=True)
        
        date_range = pd.date_range(start=df['date'].min(), end=datetime.now().date(), freq='D')
        df = df.set_index('date').reindex(date_range).reset_index().rename(columns={'index': 'date'})
        
        df = df.interpolate(method='time').ffill().bfill()
        
        df['cases_per_100k'] = (df['dhaka_cases'] / df['population_dhaka']) * 100000
        
        print(f"âœ“  Unified dataset created with {len(df)} daily records from {df['date'].min().date()} to {df['date'].max().date()}.")
        df.to_csv("./data/processed/hawkeye_omega_unified_daily.csv", index=False)
        print("âœ“  Saved unified dataset to './data/processed/'.")
        return df.dropna()

# --- INTELLIGENCE ENGINE ---
class IntelligenceEngine:
    """Contains the ML models and analysis methods."""
    def __init__(self, config):
        self.config = config
        self.causal_graph = None
        self.causal_strengths = {}
        self.gp_model = None
        self.scaler = MinMaxScaler()

    def discover_causality(self, df):
        """Computes a causal graph using lagged correlations."""
        print("4. Discovering causal relationships...")
        
        series_to_analyze = {
            'cases_per_100k': df['cases_per_100k'],
            'temperature': df['temperature'],
            'humidity': df['humidity'],
            'rainfall': df['rainfall'],
            'nightlight_radiance': df['nightlight_radiance'],
        }
        
        edges = []
        for cause_name, cause_data in series_to_analyze.items():
            for effect_name, effect_data in series_to_analyze.items():
                if cause_name != effect_name:
                    max_corr, best_lag = 0, 0
                    for lag in range(1, self.config.CAUSAL_LAG + 1):
                        corr = cause_data.corr(effect_data.shift(-lag))
                        if abs(corr) > abs(max_corr):
                            max_corr, best_lag = corr, lag
                    
                    if abs(max_corr) > self.config.CORR_THRESHOLD:
                        edges.append((cause_name, effect_name))
                        self.causal_strengths[(cause_name, effect_name)] = {'corr': max_corr, 'lag_days': best_lag}

        G = nx.DiGraph(edges)
        while not nx.is_directed_acyclic_graph(G):
            try:
                cycle = nx.find_cycle(G, orientation='original')
                weakest_edge = min(cycle, key=lambda edge: abs(self.causal_strengths.get(edge, {'corr': 0})['corr']))
                G.remove_edge(weakest_edge[0], weakest_edge[1])
            except nx.NetworkXNoCycle:
                break
        
        self.causal_graph = G
        print(f"âœ“  Causal graph discovered with {len(self.causal_graph.edges())} potential links.")
        return self.causal_graph
    
    def train_and_predict(self, df):
        """Trains a Gaussian Process model and generates a forecast."""
        print("5. Training predictive model and generating forecast...")
        
        df['day_of_year'] = df['date'].dt.dayofyear
        features = ['day_of_year', 'temperature', 'humidity', 'rainfall', 'nightlight_radiance']
        target = 'cases_per_100k'
        
        X = df[features].values
        y = df[target].values
        
        X_scaled = self.scaler.fit_transform(X)
        
        kernel = ConstantKernel(1.0) * RBF(length_scale=1.0) + WhiteKernel(noise_level=0.1)
        self.gp_model = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9, random_state=42)
        self.gp_model.fit(X_scaled, y)
        
        last_day = df['day_of_year'].iloc[-1]
        future_days = np.arange(last_day + 1, last_day + 1 + self.config.FORECAST_HORIZON)
        
        last_features = df[features].iloc[-1].values
        X_forecast = np.array([np.concatenate(([day % 365], last_features[1:])) for day in future_days])
        X_forecast_scaled = self.scaler.transform(X_forecast)

        y_pred, y_std = self.gp_model.predict(X_forecast_scaled, return_std=True)
        
        forecast_dates = pd.date_range(df['date'].max() + timedelta(days=1), periods=self.config.FORECAST_HORIZON)
        
        print("âœ“  14-day forecast generated.")
        return {
            'dates': forecast_dates.strftime('%Y-%m-%d').tolist(),
            'predicted_cases_per_100k': np.maximum(0, y_pred).tolist(),
            'uncertainty_std': y_std.tolist(),
            'confidence': (1 / (1 + y_std.mean())) * 100
        }
        
    def generate_report(self, df, forecast):
        """Generates the final JSON report and summary."""
        print("6. Generating final analysis report...")
        report = {
            'metadata': {
                'report_generated_utc': datetime.utcnow().isoformat(),
                'system_version': 'HawkEye Omega v2.1 Final',
                'location': 'Chattogram, Bangladesh',
                'local_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            },
            'disclaimer': "This analysis is based on a combination of publicly available historical data and live API feeds. Some datasets have been synthesized or interpolated to create a complete time series. This model is a proof-of-concept and should be used for informational purposes only.",
            'causal_summary': {
                'discovered_links': len(self.causal_graph.edges()),
                'strongest_drivers_of_disease': sorted([
                    {'cause': u, 'effect': v, **self.causal_strengths.get((u, v), {})}
                    for u, v in self.causal_graph.edges() if v == 'cases_per_100k'
                ], key=lambda x: abs(x.get('corr', 0)), reverse=True)
            },
            'forecast_14_day': forecast,
            'current_status': {
                'latest_data_date': df['date'].max().strftime('%Y-%m-%d'),
                'latest_cases_per_100k': df['cases_per_100k'].iloc[-1],
                'trend_7_day_change': (df['cases_per_100k'].iloc[-1] - df['cases_per_100k'].iloc[-8])
            },
            'economic_nexus': {
                'correlation_cases_vs_nightlights': df['cases_per_100k'].corr(df['nightlight_radiance']),
            }
        }
        
        with open("hawkeye_omega_final_report.json", "w") as f:
            json.dump(report, f, indent=2, default=str)
        print("âœ“  Final report saved to hawkeye_omega_final_report.json")
        return report

# --- MAIN EXECUTION ---
if __name__ == "__main__":
    
    config = Config()
    data_manager = DataManager(config)
    engine = IntelligenceEngine(config)
    
    try:
        # Execute the full pipeline
        dengue_df, weather_df, nightlight_df, pop_df = data_manager.load_historical_data()
        live_weather = data_manager.fetch_live_data()
        unified_df = data_manager.unify_data(dengue_df, weather_df, nightlight_df, pop_df, live_weather)
        
        engine.discover_causality(unified_df)
        forecast_results = engine.train_and_predict(unified_df)
        final_report = engine.generate_report(unified_df, forecast_results)
        
        # Print a clean summary to the console
        print("\n" + "="*70)
        print("      ðŸ”¬ HAWKEYE OMEGA v2 - FINAL ANALYSIS SUMMARY ðŸ”¬")
        print(f"      Report generated at {final_report['metadata']['local_time']}")
        print(f"      Location: {final_report['metadata']['location']}")
        print("="*70)
        print("\n--- Causal Insights ---")
        drivers = final_report['causal_summary']['strongest_drivers_of_disease']
        if drivers:
            for driver in drivers[:3]:
                print(f"  - {driver['cause']} is a potential driver of disease (Lag: {driver['lag_days']} days, Corr: {driver['corr']:.2f})")
        else:
            print("  - No strong causal drivers of disease found in the data.")
            
        print("\n--- 14-Day Dengue Forecast (Cases per 100k) ---")
        peak_cases = max(forecast_results['predicted_cases_per_100k'])
        peak_day_index = np.argmax(forecast_results['predicted_cases_per_100k'])
        peak_date = forecast_results['dates'][peak_day_index]
        print(f"  ðŸ“ˆ OUTBREAK PEAKING: Predicted peak of {peak_cases:.2f} cases/100k around {peak_date}.")
        print(f"  - Forecast Confidence: {forecast_results['confidence']:.1f}%")

        print("\n--- Disease-Economy Nexus ---")
        print(f"  - Correlation between cases and nightlight activity: {final_report['economic_nexus']['correlation_cases_vs_nightlights']:.3f}")
        
        print("\n" + "="*70)
        print("âœ… ANALYSIS COMPLETE. Full details in 'hawkeye_omega_final_report.json'.")
        print("="*70)
        
    except Exception as e:
        print("\n" + "!"*70)
        print("ðŸ”¥ AN ERROR OCCURRED DURING THE PIPELINE EXECUTION ðŸ”¥")
        print(f"   Error Type: {type(e).__name__}")
        print(f"   Details: {e}")
        print("   Please check file paths and API key validity.")
        print("!"*70)

```

### **Final Execution Workflow**

You are ready. Here is the final workflow.

1.  **Run the Restructuring Commands:** If you haven't already, run the `bash` commands from my previous message to organize your folders.

2.  **Run the PDF Extractor:** If you have downloaded the dengue PDFs, run your `dengue_pdf_extractor.py` script to generate the processed `bangladesh_dengue_cases_2022_2025.csv` file.

3.  **Run the Final Analysis:** From the root of your project, execute the main script:

    ```bash
    python run_hawkeye_omega.py
    ```

This single command will now execute the entire pipeline. It will load all your prepared data, fetch the latest weather, unify the time series, discover causal links, train the model, generate a forecast, and save the final report.

This is the culmination of the project.